from yarl import URL
import urllib.parse
import logging

black_list = ['www.hello.com', 'www.youtube.com']
class UrlManager:
    """
    Manages URL-related safety logic for the proxy, including blacklist 
    verification, malicious site detection, and URL normalization for 
    search redirection.
    """
    
    def is_blacklisted(self, url: str) -> bool:
        """
        Checks if a given URL matches any host in the proxy's blacklist.
        The function supports "www." identification, host-only blocking and explicit 
        path-based blocking.

        "www." prefix:
        The function handles both www. pre-fixed URLS and regular URLS.
        Examples: 
        - If www.example.com is blacklisted, the functions returns True
        for www.example.com and example.com.
        - If example.com is blacklisted, the functions returns True
        for www.example.com and example.com.

        Generally, there are 2 types of valid URLs in the blacklist:
        - General: only host, no path - the function return True for every URL
        that starts with that.
        Example: If example.com is blacklisted, the function returns True for example.com/*

        - Explicit: host and path included. In this case, the proxy will only
        block the explicit url, but allow other paths for the same host.
        Example: If example.com/index.html is blocked, the Function returns False for
        example.com and example.con/robots.txt.

        :type url: str
        :param url: The raw URL or hostname string to check.

        :rtype: bool
        :returns: True if the URL starts with a blacklisted entry, False otherwise.
        """
        url = url.lower()
        if url.startswith("www."):
            url = url[4:]

        
        for bl in black_list:
            if url.startswith(bl) or f"www.{url}".startswith(bl): # www.example.com or example.com
                return True
        return False
    
    "NO IMPLEMETATION"
    def is_malicious(self, url: str) -> bool:
        """
        Evaluates a URL malice using {library}
        to prevent access to phishing or malware-hosting sites.

        :type url: str
        :param url: The URL to be analyzed for malicious intent.

        :rtype: bool
        :returns: True if the URL is deemed dangerous, False otherwise.
        """
        return False
    
    "NOT FINISHED"
    def get_google_url(self, search_str: str) -> str:
        """
        Converts an invalid host string (often Punycode generated by browsers 
        during search queries in the address bar) into a valid Google Search URL.
        Handles IDNA decoding (Punycode) and URL-encoded characters.

        :type search_str: str
        :param search_str: The raw host string requested by the client.

        :rtype: str
        :returns: A valid Google search URL string, with the host requested as the query.

        :raises Exception: If the Punycode decoding or URL generation fails.
        """
        decoded_str = urllib.parse.unquote(search_str)
        try:
            if decoded_str.startswith("xn--"):
                # remove any spaces encoded
                search_str.replace("%20", "")
                
                # Decode with punycode
                search_str = search_str.encode().decode("idna")
                search_query = URL("https://www.google.com/search").with_query(q=search_str)
                logging.info(f"final - {search_query.query_string}")
                return str(search_query)
            return 
        except Exception as e:
            raise Exception(f"Failed generating google search URL for {search_str} - {e}") from e