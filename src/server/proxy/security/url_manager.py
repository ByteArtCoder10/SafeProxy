import logging

from yarl import URL
import urllib.parse
from ...db.sql_auth_manager import SQLAuthManager
from ...logs.loggers import core_logger
class UrlManager:
    """
    Manages URL-related safety logic for the proxy, including blacklist 
    verification, malicious site detection, and URL normalization for 
    search redirection.
    """

    db = SQLAuthManager()
    
    @staticmethod
    def is_blacklisted(url_list: list[str] , username: str) -> bool:
        """
        Checks if a given URL matches any host in the proxy's blacklist, using a db query for 
        the blacklist of a specific user.
        The function supports "www." identification, host-only blocking and explicit 
        path-based blocking.

        "www." prefix:
        The function handles both www. pre-fixed URLS and regular URLS.
        Examples: 
        - If www.example.com is blacklisted, the functions returns True
        for www.example.com and example.com.
        - If example.com is blacklisted, the functions returns True
        for www.example.com and example.com.

        Generally, there are 2 types of valid URLs in the blacklist:
        - General: only host, no path - the function return True for every URL
        that starts with that.
        Example: If example.com is blacklisted, the function returns True for example.com/*

        - Explicit: host and path included. In this case, the proxy will only
        block the explicit url, but allow other paths for the same host.
        Example: If example.com/index.html is blocked, the Function returns False for
        example.com and example.con/robots.txt.

        :param url: The raw URL or hostname string to check.
        :param username: The username's blacklist to check against.

        :rtype: bool
        :returns: True if the URL starts with a blacklisted entry, False otherwise.
        """
        blacklist = UrlManager.db.get_blacklist(username)
        core_logger.debug(f"Blacklist in URL manager: {blacklist}")
        core_logger.debug(f"URL list: {url_list}")
        if blacklist == {} or not blacklist:
            # bl == {} - no blacklist configured.
            # not bl - db error occured. assuming it is not blacklisted.
            core_logger.debug(f"Blacklst was empty. Returning False")
            return False
        
        for url in url_list:
            if url.startswith("www."):
                url = url[4:]

            core_logger.debug(f"Current URL checked: {url}")
            for bl in blacklist.keys():
                core_logger.debug(f"BL -> {bl} URL -> {url}")
                
                if url.startswith(bl):
                    # 5 possible edge-cases:
                    # 1. BL= x.com, URL = x.com => returns blacklisted
                    # 2. BL= x.com, URL = x.com/* => returns blacklisted
                    # 3. BL= x.com/*, URL = x.com => returns NOT blacklisted
                    # 4. BL= x.com/*, URL = x.com/* => returns blacklisted
                    # 5. BL= x.com/hello (long), URL = x.com/yes (short) => returns NOT blacklisted
                    if len(url) == len(bl) or url[len(bl)] in ['/', '\\', '?', '#']:
                        core_logger.debug(f"BL -> {bl} URL -> {url} matched. Returning blacklisted")
                        return True
            return False
    

    @staticmethod
    def is_malicious(url: str) -> bool:
        """
        Evaluates a URL malice using {library}
        to prevent access to phishing or malware-hosting sites.

        :type url: str
        :param url: The URL to be analyzed for malicious intent.

        :rtype: bool
        :returns: True if the URL is deemed dangerous, False otherwise.
        """
        return False
    
    @staticmethod
    def get_google_url(search_str: str) -> str:
        """
        Converts an invalid host string (often Punycode generated by browsers 
        during search queries in the address bar) into a valid Google Search URL.
        Handles IDNA decoding (Punycode) and URL-encoded characters.

        :type search_str: str
        :param search_str: The raw host string requested by the client.

        :rtype: str
        :returns: A valid Google search URL string, with the host requested as the query.

        :raises Exception: If the Punycode decoding or URL generation fails.
        """
        decoded_str = urllib.parse.unquote(search_str)
        try:
            if decoded_str.startswith("xn--"):
                # remove any spaces encoded
                search_str.replace("%20", "")
                
                # Decode with punycode
                search_str = search_str.encode().decode("idna")
                search_query = URL("https://www.google.com/search").with_query(q=search_str)
                logging.info(f"final - {search_query.query_string}")
                return str(search_query)
            return 
        except Exception as e:
            raise Exception(f"Failed generating google search URL for {search_str} - {e}") from e